# Llama 3.1 8B with QLoRA (Quantized LoRA)
# Memory-efficient: Uses 4-bit quantization
# Great for GPUs with less memory (< 24GB)

model:
  model_name: "meta-llama/Llama-3.1-8B-Instruct"
  model_max_length: 8192
  dtype: "bfloat16"
  attn_implementation: "flash_attention_2"
  use_cache: false

  # Quantization settings
  load_in_4bit: true
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_compute_dtype: "bfloat16"
  bnb_4bit_use_double_quant: true

data:
  train_dataset:
    dataset_name: "yahma/alpaca-cleaned"
    split: "train"
  target_column: "text"
  template: "llama3"

training:
  trainer_type: "sft"
  per_device_train_batch_size: 4  # Can use larger batch with QLoRA
  gradient_accumulation_steps: 16  # Effective batch size: 4 * 16 = 64
  learning_rate: 2.0e-04
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.03
  num_train_epochs: 3
  save_steps: 50
  logging_steps: 5
  save_total_limit: 3
  output_dir: "./output/llama8b_qlora"
  bf16: true
  gradient_checkpointing: true
  optim: "paged_adamw_32bit"  # Memory-efficient optimizer for QLoRA

  # Evaluation
  eval_strategy: "steps"
  eval_steps: 50

  # Logging
  report_to: "none"

peft:
  type: "qlora"
  lora_r: 16  # Slightly higher rank for QLoRA
  lora_alpha: 32
  lora_dropout: 0.05
  lora_target_modules: ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
  bias: "none"
  task_type: "CAUSAL_LM"
