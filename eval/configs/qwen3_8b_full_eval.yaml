# Comprehensive evaluation for Qwen3 8B
# Tests across multiple standard benchmarks

model:
  model_name: "Qwen/Qwen3-8B"
  # Or use your fine-tuned checkpoint:
  # model_name: "./output/qwen3_8b_lora"
  model_max_length: 32768
  dtype: "bfloat16"
  attn_implementation: "flash_attention_2"

generation:
  per_device_batch_size: 4
  temperature: 0.0
  max_new_tokens: 512

tasks:
  # Knowledge benchmarks
  - backend: "lm_harness"
    task: "mmlu"
    num_fewshot: 5
    output_path: "./eval_results/qwen3_8b/mmlu"

  # Math reasoning
  - backend: "lm_harness"
    task: "gsm8k"
    num_fewshot: 8
    output_path: "./eval_results/qwen3_8b/gsm8k"

  # Commonsense reasoning
  - backend: "lm_harness"
    task: "hellaswag"
    num_fewshot: 10
    output_path: "./eval_results/qwen3_8b/hellaswag"

  # Truthfulness
  - backend: "lm_harness"
    task: "truthfulqa_mc2"
    num_fewshot: 0
    output_path: "./eval_results/qwen3_8b/truthfulqa"
